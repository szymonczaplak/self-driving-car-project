{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Szymon\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Szymon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Szymon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Szymon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Szymon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Szymon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Szymon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import keras \n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398.0</td>\n",
       "      <td>101.857902</td>\n",
       "      <td>60.925663</td>\n",
       "      <td>474.156302</td>\n",
       "      <td>231.819254</td>\n",
       "      <td>straight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>396.0</td>\n",
       "      <td>101.542193</td>\n",
       "      <td>60.925663</td>\n",
       "      <td>472.084688</td>\n",
       "      <td>231.819254</td>\n",
       "      <td>straight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>394.0</td>\n",
       "      <td>101.542193</td>\n",
       "      <td>60.925663</td>\n",
       "      <td>470.015662</td>\n",
       "      <td>231.819254</td>\n",
       "      <td>straight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>392.0</td>\n",
       "      <td>101.542193</td>\n",
       "      <td>60.925663</td>\n",
       "      <td>467.944048</td>\n",
       "      <td>231.819254</td>\n",
       "      <td>straight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>390.0</td>\n",
       "      <td>101.542193</td>\n",
       "      <td>60.925663</td>\n",
       "      <td>465.875022</td>\n",
       "      <td>231.819254</td>\n",
       "      <td>straight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sensor_1    sensor_2   sensor_3    sensor_4    sensor_5  decision\n",
       "0     398.0  101.857902  60.925663  474.156302  231.819254  straight\n",
       "1     396.0  101.542193  60.925663  472.084688  231.819254  straight\n",
       "2     394.0  101.542193  60.925663  470.015662  231.819254  straight\n",
       "3     392.0  101.542193  60.925663  467.944048  231.819254  straight\n",
       "4     390.0  101.542193  60.925663  465.875022  231.819254  straight"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'decision']\n",
    "data = pd.read_csv('output_cumulated.csv', names=headers)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data, columns=['decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15003 entries, 0 to 15002\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sensor_1           15003 non-null  float64\n",
      " 1   sensor_2           15003 non-null  float64\n",
      " 2   sensor_3           15003 non-null  float64\n",
      " 3   sensor_4           15003 non-null  float64\n",
      " 4   sensor_5           15003 non-null  float64\n",
      " 5   decision_left      15003 non-null  uint8  \n",
      " 6   decision_right     15003 non-null  uint8  \n",
      " 7   decision_straight  15003 non-null  uint8  \n",
      "dtypes: float64(5), uint8(3)\n",
      "memory usage: 630.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[list(df.columns)[:5]]\n",
    "y = df[list(df.columns)[5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, shuffle=True, test_size=0.33, random_state=42, stratify=data['decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = np.clip(X_train, -5, 5)\n",
    "X_test = np.clip(X_test, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights={0: 1 - sum([el[0] for el in y_train]) / len(y_train), # left\n",
    "#                1: 1 - sum([el[1] for el in y_train]) / len(y_train), # right\n",
    "#                2: 1 - sum([el[2] for el in y_train]) / len(y_train), # straight\n",
    "#               }\n",
    "class_weights={0: 10, # left\n",
    "               1: 10, # right\n",
    "               2: 1, # straight\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 10, 1: 10, 2: 1}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    network = keras.Sequential()\n",
    "    network.add(Dense(2000, activation='relu', input_shape=(5,)))\n",
    "    network.add(Dropout(0.15))\n",
    "    network.add(Dense(2000, activation='relu', input_shape=(5,)))\n",
    "    network.add(Dropout(0.15))\n",
    "    network.add(Dense(3, activation='softmax'))\n",
    "    optimizer = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    network.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "def build_model_2():\n",
    "    K.clear_session()\n",
    "    inputs = Input(shape=(5,1))\n",
    "    #First Conv1D layer\n",
    "    conv = Conv1D(45,3, padding='valid', activation='relu', strides=1)(inputs)\n",
    "    conv = MaxPooling1D(3)(conv)\n",
    "    conv = Dropout(0.3)(conv)\n",
    "    \n",
    "    #Flatten layer\n",
    "    conv = Flatten()(conv)\n",
    "\n",
    "    #Dense Layer 1\n",
    "    conv = Dense(4500, activation='relu')(conv)\n",
    "    conv = Dropout(0.3)(conv)\n",
    "    outputs = Dense(3, activation='softmax')(conv)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprob',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model_f, train_data, train_targets, epochs, class_weights):\n",
    "    k = 5\n",
    "    num_val_samples = len(train_data) // k\n",
    "    num_epochs = epochs\n",
    "    all_scores = []\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        # Prepare the validation data: data from partition # k\n",
    "        val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        # Prepare the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate(\n",
    "            [train_data[:i * num_val_samples],\n",
    "             train_data[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [train_targets[:i * num_val_samples],\n",
    "             train_targets[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "\n",
    "        # Build the Keras model (already compiled)\n",
    "        model = model_f()\n",
    "        # Train the model (in silent mode, verbose=0)\n",
    "        history = model.fit(partial_train_data, partial_train_targets,\n",
    "                  epochs=num_epochs, batch_size=32, verbose=1, class_weight=class_weights, validation_data=(val_data, val_targets))\n",
    "        # Evaluate the model on the validation data\n",
    "        val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n",
    "        all_scores.append(val_mae)\n",
    "        pred = model.predict(val_data)\n",
    "        \n",
    "        print(f\"Mean absolute error: {val_mae}\\n\")\n",
    "        \n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.plot(history.history['loss'], 'bo')\n",
    "        plt.plot(history.history['val_loss'], 'b')\n",
    "        plt.title('Whole training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        #plt.ylim((0, 15))\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "    return mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 11794 samples, validate on 2948 samples\n",
      "Epoch 1/50\n",
      "11794/11794 [==============================] - 1s 125us/step - loss: 0.6286 - accuracy: 0.7622 - val_loss: 0.6130 - val_accuracy: 0.7619\n",
      "Epoch 2/50\n",
      "11794/11794 [==============================] - 1s 83us/step - loss: 0.5894 - accuracy: 0.7705 - val_loss: 0.6074 - val_accuracy: 0.7659\n",
      "Epoch 3/50\n",
      "11794/11794 [==============================] - 1s 83us/step - loss: 0.5805 - accuracy: 0.7712 - val_loss: 0.5945 - val_accuracy: 0.7636\n",
      "Epoch 4/50\n",
      "11794/11794 [==============================] - 1s 83us/step - loss: 0.5706 - accuracy: 0.7758 - val_loss: 0.5768 - val_accuracy: 0.7710\n",
      "Epoch 5/50\n",
      "11794/11794 [==============================] - 1s 83us/step - loss: 0.5664 - accuracy: 0.7757 - val_loss: 0.5704 - val_accuracy: 0.7727\n",
      "Epoch 6/50\n",
      "11794/11794 [==============================] - 1s 83us/step - loss: 0.5629 - accuracy: 0.7752 - val_loss: 0.5889 - val_accuracy: 0.7697\n",
      "Epoch 7/50\n",
      "11794/11794 [==============================] - 1s 87us/step - loss: 0.5558 - accuracy: 0.7810 - val_loss: 0.5731 - val_accuracy: 0.7714\n",
      "Epoch 8/50\n",
      "11794/11794 [==============================] - 1s 84us/step - loss: 0.5538 - accuracy: 0.7799 - val_loss: 0.5865 - val_accuracy: 0.7741\n",
      "Epoch 9/50\n",
      "11794/11794 [==============================] - 1s 84us/step - loss: 0.5491 - accuracy: 0.7810 - val_loss: 0.5748 - val_accuracy: 0.7632\n",
      "Epoch 10/50\n",
      "11794/11794 [==============================] - 1s 87us/step - loss: 0.5451 - accuracy: 0.7829 - val_loss: 0.5873 - val_accuracy: 0.7649\n",
      "Epoch 11/50\n",
      "11794/11794 [==============================] - 1s 84us/step - loss: 0.5437 - accuracy: 0.7825 - val_loss: 0.5790 - val_accuracy: 0.7748\n",
      "Epoch 12/50\n",
      "11794/11794 [==============================] - 1s 85us/step - loss: 0.5391 - accuracy: 0.7828 - val_loss: 0.5688 - val_accuracy: 0.7799\n",
      "Epoch 13/50\n",
      "11794/11794 [==============================] - 1s 85us/step - loss: 0.5378 - accuracy: 0.7844 - val_loss: 0.5544 - val_accuracy: 0.7778\n",
      "Epoch 14/50\n",
      "11794/11794 [==============================] - 1s 88us/step - loss: 0.5345 - accuracy: 0.7858 - val_loss: 0.5629 - val_accuracy: 0.7700\n",
      "Epoch 15/50\n",
      "11794/11794 [==============================] - 1s 88us/step - loss: 0.5328 - accuracy: 0.7851 - val_loss: 0.5721 - val_accuracy: 0.7680\n",
      "Epoch 16/50\n",
      "11794/11794 [==============================] - 1s 85us/step - loss: 0.5307 - accuracy: 0.7883 - val_loss: 0.5444 - val_accuracy: 0.7788\n",
      "Epoch 17/50\n",
      "11794/11794 [==============================] - 1s 84us/step - loss: 0.5296 - accuracy: 0.7888 - val_loss: 0.5417 - val_accuracy: 0.7877\n",
      "Epoch 18/50\n",
      "11794/11794 [==============================] - 1s 88us/step - loss: 0.5239 - accuracy: 0.7898 - val_loss: 0.5486 - val_accuracy: 0.7849\n",
      "Epoch 19/50\n",
      "11794/11794 [==============================] - 1s 85us/step - loss: 0.5267 - accuracy: 0.7864 - val_loss: 0.5406 - val_accuracy: 0.7863\n",
      "Epoch 20/50\n",
      "11794/11794 [==============================] - 1s 85us/step - loss: 0.5250 - accuracy: 0.7919 - val_loss: 0.5406 - val_accuracy: 0.7843\n",
      "Epoch 21/50\n",
      "11794/11794 [==============================] - 1s 88us/step - loss: 0.5234 - accuracy: 0.7885 - val_loss: 0.5483 - val_accuracy: 0.7846\n",
      "Epoch 22/50\n",
      "11794/11794 [==============================] - 1s 86us/step - loss: 0.5221 - accuracy: 0.7912 - val_loss: 0.5385 - val_accuracy: 0.7799\n",
      "Epoch 23/50\n",
      "11794/11794 [==============================] - 1s 85us/step - loss: 0.5187 - accuracy: 0.7933 - val_loss: 0.5426 - val_accuracy: 0.7849\n",
      "Epoch 24/50\n",
      "11794/11794 [==============================] - 1s 98us/step - loss: 0.5170 - accuracy: 0.7928 - val_loss: 0.5457 - val_accuracy: 0.7775\n",
      "Epoch 25/50\n",
      "11794/11794 [==============================] - 1s 110us/step - loss: 0.5191 - accuracy: 0.7907 - val_loss: 0.5293 - val_accuracy: 0.7809\n",
      "Epoch 26/50\n",
      "11794/11794 [==============================] - 1s 91us/step - loss: 0.5176 - accuracy: 0.7924 - val_loss: 0.5344 - val_accuracy: 0.7826\n",
      "Epoch 27/50\n",
      "11794/11794 [==============================] - 1s 90us/step - loss: 0.5133 - accuracy: 0.7928 - val_loss: 0.5348 - val_accuracy: 0.7873\n",
      "Epoch 28/50\n",
      "11488/11794 [============================>.] - ETA: 0s - loss: 0.5130 - accuracy: 0.7951"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-d93cb2c42e22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluate_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-a8e97fd5c74a>\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[1;34m(model_f, train_data, train_targets, epochs, class_weights)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Train the model (in silent mode, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         history = model.fit(partial_train_data, partial_train_targets,\n\u001b[1;32m---> 26\u001b[1;33m                   epochs=num_epochs, batch_size=32, verbose=1, class_weight=class_weights, validation_data=(val_data, val_targets))\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Evaluate the model on the validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mval_mse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                                          verbose=0)\n\u001b[0m\u001b[0;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                     \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "evaluate_classifier(build_model, X_train, y_train, 50, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10052 samples, validate on 4951 samples\n",
      "Epoch 1/200\n",
      "10052/10052 [==============================] - 3s 266us/step - loss: 0.3533 - accuracy: 0.8547 - val_loss: 0.4681 - val_accuracy: 0.8352\n",
      "Epoch 2/200\n",
      "10052/10052 [==============================] - 2s 237us/step - loss: 0.3493 - accuracy: 0.8526 - val_loss: 0.4601 - val_accuracy: 0.8406\n",
      "Epoch 3/200\n",
      "10052/10052 [==============================] - 2s 235us/step - loss: 0.3514 - accuracy: 0.8522 - val_loss: 0.4626 - val_accuracy: 0.8352\n",
      "Epoch 4/200\n",
      "10052/10052 [==============================] - 2s 235us/step - loss: 0.3496 - accuracy: 0.8522 - val_loss: 0.4596 - val_accuracy: 0.8330\n",
      "Epoch 5/200\n",
      "10052/10052 [==============================] - 2s 235us/step - loss: 0.3491 - accuracy: 0.8534 - val_loss: 0.4701 - val_accuracy: 0.8338\n",
      "Epoch 6/200\n",
      "10052/10052 [==============================] - 2s 246us/step - loss: 0.3503 - accuracy: 0.8559 - val_loss: 0.4713 - val_accuracy: 0.8402\n",
      "Epoch 7/200\n",
      "10052/10052 [==============================] - 2s 236us/step - loss: 0.3522 - accuracy: 0.8548 - val_loss: 0.4674 - val_accuracy: 0.8380\n",
      "Epoch 8/200\n",
      "10052/10052 [==============================] - 2s 236us/step - loss: 0.3469 - accuracy: 0.8561 - val_loss: 0.4646 - val_accuracy: 0.8374\n",
      "Epoch 9/200\n",
      "10052/10052 [==============================] - 2s 237us/step - loss: 0.3457 - accuracy: 0.8558 - val_loss: 0.4799 - val_accuracy: 0.8346\n",
      "Epoch 10/200\n",
      "10052/10052 [==============================] - 2s 239us/step - loss: 0.3488 - accuracy: 0.8522 - val_loss: 0.4762 - val_accuracy: 0.8348\n",
      "Epoch 11/200\n",
      " 2816/10052 [=======>......................] - ETA: 1s - loss: 0.3350 - accuracy: 0.8580"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-068ad7deb0f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 382   10  386]\n",
      " [   6  304  258]\n",
      " [  80   56 3469]]\n"
     ]
    }
   ],
   "source": [
    "pred_labels = [np.argmax(el) for el in predictions]\n",
    "true_labels = [np.argmax(el) for el in y_test]\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"Confusion matrix:\\n{}\".\\\n",
    "      format(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_model_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
